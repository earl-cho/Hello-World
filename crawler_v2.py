import feedparser
import requests
from supabase import create_client
import os
from dotenv import load_dotenv
from datetime import datetime

# ---------------------------------------------------------
# [ì„¤ì •] í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
# ---------------------------------------------------------
load_dotenv()
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

if not SUPABASE_URL or not SUPABASE_KEY:
    print("âŒ .env íŒŒì¼ í™•ì¸ í•„ìš”")
    exit()

supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

# ---------------------------------------------------------
# [ì„¤ì •] ê°€ì§œ ì‹ ë¶„ì¦ (User-Agent)
# ---------------------------------------------------------
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# ---------------------------------------------------------
# [íƒ€ê²Ÿ ì†ŒìŠ¤] ë§¤ì¼ê²½ì œ ì¦ê¶Œ ì„¹ì…˜ ì¶”ê°€ë¨!
# ---------------------------------------------------------
RSS_FEEDS = [
    {
        "source": "CoinDesk (Policy)", 
        "url": "https://www.coindesk.com/arc/outboundfeeds/rss/?outputType=xml&tag=policy",
        "must_filter": False 
    },
    {
        "source": "The Block", 
        "url": "https://www.theblock.co/rss.xml", 
        "must_filter": True 
    },
    {
        "source": "Ledger Insights", 
        "url": "https://www.ledgerinsights.com/feed/", 
        "must_filter": True
    },
    {
        "source": "ë§¤ì¼ê²½ì œ (ê²½ì œ)", 
        "url": "https://www.mk.co.kr/rss/30100041/",
        "must_filter": True 
    },
    {
        "source": "ë§¤ì¼ê²½ì œ (ì¦ê¶Œ)", 
        "url": "https://www.mk.co.kr/rss/40200003/", # [ì¶”ê°€] ì½”ì¸ ë‰´ìŠ¤ëŠ” ì—¬ê¸°ì— ë§ìŠµë‹ˆë‹¤
        "must_filter": True 
    }
]

# ---------------------------------------------------------
# [í•µì‹¬] í‚¤ì›Œë“œ ê·¸ë¬¼ë§
# ---------------------------------------------------------
KEYWORDS = [
    # 1. ê·œì œ/ê¸°ê´€
    "sec", "fsc", "fsa", "regulation", "law", "ban", "tax", "policy", "legal", "compliance",
    "ê·œì œ", "ê¸ˆìœµìœ„", "ê¸ˆê°ì›", "ë²•ì•ˆ", "ì„¸ê¸ˆ", "ê³¼ì„¸", "ìŠ¹ì¸", "ê¸°ì†Œ", "íŒê²°",
    
    # 2. ìì‚°/í† í°
    "crypto", "bitcoin", "ethereum", "stablecoin", "cbdc", "sto", "rwa", "token", "digital asset", "virtual asset",
    "í¬ë¦½í† ", "ë¹„íŠ¸ì½”ì¸", "ì´ë”ë¦¬ì›€", "ê°€ìƒìì‚°", "ë””ì§€í„¸ìì‚°", "í† í°", "ì¦ê¶Œí˜•", "í˜„ë¬¼", "ë¸”ë¡ì²´ì¸"
]

def save_to_db(category, source, title, link, date):
    data = {
        "category": category,
        "source_name": source,
        "title": title,
        "url": link,
        "published_date": str(date)
    }
    
    try:
        response = supabase.table("raw_intelligence").select("url").eq("url", link).execute()
        if not response.data: 
            supabase.table("raw_intelligence").insert(data).execute()
            print(f"âœ… [ì €ì¥] {title[:30]}...") 
        else:
            pass 
    except Exception as e:
        print(f"âš ï¸ ì €ì¥ ì—ëŸ¬: {e}")

def run_crawler():
    print("ğŸ•µï¸ ë¸”ë™ë³´ë“œ ìˆ˜ì§‘ ì—”ì§„ v2 ê°€ë™ (ë§¤ê²½ ì¦ê¶Œ ì¶”ê°€ë¨)...")
    
    for feed in RSS_FEEDS:
        print(f"\nğŸ“¡ [{feed['source']}] ì ‘ì† ì‹œë„...")
        
        try:
            response = requests.get(feed['url'], headers=HEADERS, timeout=15)
            
            if response.status_code != 200:
                print(f"âŒ ì ‘ì† ì‹¤íŒ¨ (ì½”ë“œ: {response.status_code})")
                continue

            parsed_feed = feedparser.parse(response.content)
            entries = parsed_feed.entries
            print(f"   -> {len(entries)}ê°œ ê¸°ì‚¬ ìŠ¤ìº” ì¤‘...")

            saved_count = 0
            for entry in entries[:15]: # ë” ë§ì´(15ê°œ) ìŠ¤ìº”
                title = entry.title
                link = entry.link
                published = entry.get('published', datetime.now().isoformat())
                
                title_lower = title.lower()
                is_match = False
                
                if not feed['must_filter']:
                    is_match = True
                else:
                    if any(k in title_lower for k in KEYWORDS):
                        is_match = True
                
                if is_match:
                    save_to_db("regulation", feed['source'], title, link, published)
                    saved_count += 1
            
            if saved_count == 0 and feed['must_filter']:
                print("   [ê²°ê³¼] ì €ì¥ëœ ê¸°ì‚¬ ì—†ìŒ (í‚¤ì›Œë“œ ë§¤ì¹­ ì‹¤íŒ¨)")

        except Exception as e:
            print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")

    print("\nğŸ‰ ìˆ˜ì§‘ ì™„ë£Œ!")

if __name__ == "__main__":
    run_crawler()
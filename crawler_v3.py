import feedparser
import requests
from supabase import create_client
import os
from dotenv import load_dotenv
from datetime import datetime

# ---------------------------------------------------------
# [ì„¤ì •] í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
# ---------------------------------------------------------
load_dotenv()
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

if not SUPABASE_URL or not SUPABASE_KEY:
    print("âŒ .env íŒŒì¼ í™•ì¸ í•„ìš”")
    exit()

supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# ---------------------------------------------------------
# [íƒ€ê²Ÿ ì†ŒìŠ¤] ë§¤ê²½(ê²½ì œ/ì¦ê¶Œ) + ë¸”ë¡ë¯¸ë””ì–´(ë³´í—˜ìš©)
# ---------------------------------------------------------
RSS_FEEDS = [
    {
        "source": "CoinDesk (Policy)", 
        "url": "https://www.coindesk.com/arc/outboundfeeds/rss/?outputType=xml&tag=policy",
        "must_filter": False 
    },
    {
        "source": "The Block", 
        "url": "https://www.theblock.co/rss.xml", 
        "must_filter": True 
    },
    {
        "source": "Ledger Insights", 
        "url": "https://www.ledgerinsights.com/feed/", 
        "must_filter": True
    },
    {
        "source": "ë§¤ì¼ê²½ì œ (ê²½ì œ)", 
        "url": "https://www.mk.co.kr/rss/30100041/",
        "must_filter": True 
    },
    {
        "source": "ë§¤ì¼ê²½ì œ (ì¦ê¶Œ)", 
        "url": "https://www.mk.co.kr/rss/40200003/", 
        "must_filter": True 
    },
    {
        "source": "BlockMedia (KR)", # ë§¤ê²½ì´ ì¡°ìš©í•  ë•Œë¥¼ ëŒ€ë¹„í•œ ë³´í—˜
        "url": "https://www.blockmedia.co.kr/feed",
        "must_filter": False # ì—¬ê¸´ ì „ìš©ì§€ë‹ˆê¹Œ í•„í„° ì—†ì´ ë‹¤ ê°€ì ¸ì˜´
    }
]

# ---------------------------------------------------------
# [í‚¤ì›Œë“œ] ì˜ì–´/í•œêµ­ì–´ ê·¸ë¬¼ë§
# ---------------------------------------------------------
KEYWORDS = [
    "sec", "fsc", "fsa", "regulation", "law", "ban", "tax", "policy", "legal",
    "crypto", "bitcoin", "ethereum", "stablecoin", "cbdc", "sto", "rwa", "token",
    "ê·œì œ", "ê¸ˆìœµ", "ê¸ˆê°ì›", "ë²•ì•ˆ", "ì„¸ê¸ˆ", "ìŠ¹ì¸", "ê¸°ì†Œ",
    "í¬ë¦½í† ", "ë¹„íŠ¸ì½”ì¸", "ì´ë”ë¦¬ì›€", "ê°€ìƒìì‚°", "ë””ì§€í„¸ìì‚°", "í† í°", "ì¦ê¶Œí˜•", "ë¸”ë¡ì²´ì¸", "ì½”ì¸"
]

def save_to_db(category, source, title, link, date, summary=""):
    # ìš”ì•½ë¬¸(summary)ë„ ê°™ì´ ì €ì¥í•˜ë©´ ë‚˜ì¤‘ì— AIê°€ ì½ê¸° ì¢‹ìŠµë‹ˆë‹¤.
    data = {
        "category": category,
        "source_name": source,
        "title": title,
        "url": link,
        "published_date": str(date),
        "summary": summary[:500] # ë„ˆë¬´ ê¸¸ë©´ ìë¦„
    }
    
    try:
        response = supabase.table("raw_intelligence").select("url").eq("url", link).execute()
        if not response.data: 
            supabase.table("raw_intelligence").insert(data).execute()
            print(f"âœ… [ì €ì¥] {title[:20]}...") 
        else:
            pass 
    except Exception as e:
        print(f"âš ï¸ ì €ì¥ ì—ëŸ¬: {e}")

def run_crawler():
    print("ğŸ•µï¸ ë¸”ë™ë³´ë“œ ìˆ˜ì§‘ ì—”ì§„ v3 ê°€ë™ (ë³¸ë¬¸ ê²€ìƒ‰ + ë¸”ë¡ë¯¸ë””ì–´)...")
    
    for feed in RSS_FEEDS:
        print(f"\nğŸ“¡ [{feed['source']}] ì ‘ì† ì‹œë„...")
        
        try:
            response = requests.get(feed['url'], headers=HEADERS, timeout=15)
            
            if response.status_code != 200:
                print(f"âŒ ì ‘ì† ì‹¤íŒ¨ (ì½”ë“œ: {response.status_code})")
                continue

            parsed_feed = feedparser.parse(response.content)
            entries = parsed_feed.entries
            print(f"   -> {len(entries)}ê°œ ê¸°ì‚¬ ìŠ¤ìº” ì¤‘...")

            saved_count = 0
            # ë§¤ê²½ ê°™ì€ ê³³ì€ ê¸°ì‚¬ê°€ ë§ìœ¼ë‹ˆ 20ê°œê¹Œì§€ í™•ì¸
            scan_limit = 20 if "ë§¤ì¼ê²½ì œ" in feed['source'] else 10

            for entry in entries[:scan_limit]: 
                title = entry.title
                link = entry.link
                published = entry.get('published', datetime.now().isoformat())
                
                # [ì—…ê·¸ë ˆì´ë“œ] ì œëª©ë¿ë§Œ ì•„ë‹ˆë¼ ìš”ì•½ê¸€(description)ë„ ê°€ì ¸ì˜´
                summary = entry.get('description', '') 
                
                # ê²€ìƒ‰ ëŒ€ìƒ: ì œëª© + ìš”ì•½ê¸€ (ì†Œë¬¸ìë¡œ ë³€í™˜)
                target_text = (title + " " + summary).lower()
                
                is_match = False
                
                if not feed['must_filter']:
                    is_match = True
                else:
                    if any(k in target_text for k in KEYWORDS):
                        is_match = True
                
                if is_match:
                    save_to_db("regulation", feed['source'], title, link, published, summary)
                    saved_count += 1
                else:
                    # [ë””ë²„ê¹…] ë§¤ì¼ê²½ì œì—ì„œ ë­˜ ë²„ë¦¬ëŠ”ì§€ ë”± 1ê°œë§Œ ìƒ˜í”Œë¡œ ë³´ì—¬ì¤Œ
                    if "ë§¤ì¼ê²½ì œ" in feed['source'] and saved_count == 0 and entries.index(entry) == 0:
                         print(f"   [íƒˆë½ ì˜ˆì‹œ] {title} (í‚¤ì›Œë“œ ì—†ìŒ)")

            if saved_count == 0 and feed['must_filter']:
                print("   [ê²°ê³¼] ì €ì¥ëœ ê¸°ì‚¬ ì—†ìŒ.")

        except Exception as e:
            print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")

    print("\nğŸ‰ ìˆ˜ì§‘ ì™„ë£Œ!")

if __name__ == "__main__":
    run_crawler()
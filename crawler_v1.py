import feedparser
import requests
from supabase import create_client
import os
from dotenv import load_dotenv
from datetime import datetime

# ---------------------------------------------------------
# [ì„¤ì •] í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
# ---------------------------------------------------------
load_dotenv()
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

if not SUPABASE_URL or not SUPABASE_KEY:
    print("âŒ .env íŒŒì¼ í™•ì¸ í•„ìš”")
    exit()

supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

# ---------------------------------------------------------
# [ì„¤ì •] ê°€ì§œ ì‹ ë¶„ì¦ (User-Agent)
# ---------------------------------------------------------
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# ---------------------------------------------------------
# [íƒ€ê²Ÿ ì†ŒìŠ¤] ìš”ì²­í•˜ì‹  4ê³³ í™•ì •
# ---------------------------------------------------------
RSS_FEEDS = [
    {
        "source": "CoinDesk (Policy)", 
        "url": "https://www.coindesk.com/arc/outboundfeeds/rss/?outputType=xml&tag=policy",
        "must_filter": False # ì´ë¯¸ Policy ì±„ë„ì´ë¼ í•„í„° ë¶ˆí•„ìš”
    },
    {
        "source": "The Block", 
        "url": "https://www.theblock.co/rss.xml", # ìˆ˜ì •: .xml ì¶”ê°€
        "must_filter": True # ì „ì²´ ë‰´ìŠ¤ë¼ í•„í„°ë§ í•„ìš”
    },
    {
        "source": "Ledger Insights", 
        "url": "https://www.ledgerinsights.com/feed/", # ì¶”ê°€: ì—”í„°í”„ë¼ì´ì¦ˆ/ê·œì œ ë§›ì§‘
        "must_filter": True
    },
    {
        "source": "ë§¤ì¼ê²½ì œ (ê²½ì œ)", 
        "url": "https://www.mk.co.kr/rss/30100041/",
        "must_filter": True # ê²½ì œ ë‰´ìŠ¤ ì¤‘ ì½”ì¸ë§Œ ê³¨ë¼ë‚´ì•¼ í•¨
    }
]

# ---------------------------------------------------------
# [í•µì‹¬] í‚¤ì›Œë“œ ê·¸ë¬¼ë§ (User ë‹ˆì¦ˆ ë°˜ì˜)
# ---------------------------------------------------------
KEYWORDS = [
    # 1. ê·œì œ/ê¸°ê´€
    "sec", "fsc", "fsa", "regulation", "law", "ban", "tax", "policy", "legal", "compliance",
    "ê·œì œ", "ê¸ˆìœµìœ„", "ê¸ˆê°ì›", "ë²•ì•ˆ", "ì„¸ê¸ˆ", "ê³¼ì„¸", "ìŠ¹ì¸", "ê¸°ì†Œ", "íŒê²°",
    
    # 2. ìì‚°/í† í° (ë§¤ì¼ê²½ì œ í•„í„°ìš©)
    "crypto", "bitcoin", "ethereum", "stablecoin", "cbdc", "sto", "rwa", "token", "digital asset", "virtual asset",
    "í¬ë¦½í† ", "ë¹„íŠ¸ì½”ì¸", "ì´ë”ë¦¬ì›€", "ê°€ìƒìì‚°", "ë””ì§€í„¸ìì‚°", "í† í°", "ì¦ê¶Œí˜•", "í˜„ë¬¼", "ë¸”ë¡ì²´ì¸"
]

def save_to_db(category, source, title, link, date):
    data = {
        "category": category,
        "source_name": source,
        "title": title,
        "url": link,
        "published_date": str(date)
    }
    
    try:
        response = supabase.table("raw_intelligence").select("url").eq("url", link).execute()
        if not response.data: 
            supabase.table("raw_intelligence").insert(data).execute()
            print(f"âœ… [ì €ì¥] {title[:30]}...") 
        else:
            pass # ì¤‘ë³µ ìƒëµ
    except Exception as e:
        print(f"âš ï¸ ì €ì¥ ì—ëŸ¬: {e}")

def run_crawler():
    print("ğŸ•µï¸ ë¸”ë™ë³´ë“œ ìˆ˜ì§‘ ì—”ì§„ v2 ê°€ë™ (ë§¤ê²½/ë”ë¸”ë¡/ë ›ì €ì¸ì‚¬ì´íŠ¸)...")
    
    for feed in RSS_FEEDS:
        print(f"\nğŸ“¡ [{feed['source']}] ì ‘ì† ì‹œë„...")
        
        try:
            # 1. ìš”ì²­
            response = requests.get(feed['url'], headers=HEADERS, timeout=15)
            
            if response.status_code != 200:
                print(f"âŒ ì ‘ì† ì‹¤íŒ¨ (ì½”ë“œ: {response.status_code}) -> URL í™•ì¸ í•„ìš”")
                continue

            # 2. íŒŒì‹±
            parsed_feed = feedparser.parse(response.content)
            entries = parsed_feed.entries
            print(f"   -> {len(entries)}ê°œ ê¸°ì‚¬ ìŠ¤ìº” ì¤‘...")

            # 3. í•„í„°ë§ ë° ì €ì¥
            saved_count = 0
            for entry in entries[:10]: # ì†ŒìŠ¤ë‹¹ ìµœì‹  10ê°œ ê²€ì‚¬
                title = entry.title
                link = entry.link
                published = entry.get('published', datetime.now().isoformat())
                
                # í•„í„° ë¡œì§
                title_lower = title.lower()
                is_match = False
                
                if not feed['must_filter']:
                    # ì½”ì¸ë°ìŠ¤í¬ Policy ê°™ì€ ì „ë¬¸ ì±„ë„ì€ ë¬´ì¡°ê±´ í†µê³¼
                    is_match = True
                else:
                    # ê·¸ ì™¸(ë§¤ê²½, ë”ë¸”ë¡ ë“±)ëŠ” í‚¤ì›Œë“œê°€ í•˜ë‚˜ë¼ë„ ìˆì–´ì•¼ í†µê³¼
                    if any(k in title_lower for k in KEYWORDS):
                        is_match = True
                
                if is_match:
                    save_to_db("regulation", feed['source'], title, link, published)
                    saved_count += 1
            
            if saved_count == 0 and feed['must_filter']:
                print("   [ê²°ê³¼] ì €ì¥ëœ ê¸°ì‚¬ ì—†ìŒ (í‚¤ì›Œë“œ ë§¤ì¹­ ì‹¤íŒ¨)")

        except Exception as e:
            print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")

    print("\nğŸ‰ ìˆ˜ì§‘ ì™„ë£Œ!")

if __name__ == "__main__":
    run_crawler()